{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from myAdagrad import myAdagrad\n",
    "from SCAdagrad import SCAdagrad\n",
    "seed =1000\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "batch_size = 4\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
    "                                        download=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
    "                                          shuffle=True, num_workers=2)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
    "                                       download=True, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n",
    "                                         shuffle=False, num_workers=2)\n",
    "\n",
    "classes = ('plane', 'car', 'bird', 'cat',\n",
    "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "\n",
    "\n",
    "\n",
    "net= torchvision.models.resnet18()\n",
    "net = net.to(device)\n",
    "\n",
    "net1= torchvision.models.resnet18()\n",
    "net1 = net1.to(device)\n",
    "\n",
    "\n",
    "import torch.optim as optim\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "#optimizer = optim.Adagrad(net.parameters(), lr=0.001)\n",
    "#optimizer  = optim.Adam(net.parameters(), lr =0.0001)\n",
    "#optimizer = optim.SGD(net.parameters(),lr = 0.001,momentum=0.9)\n",
    "optimizer = myAdagrad(net.parameters(), lr=0.01)\n",
    "#optimizer = SCAdagrad(net.parameters(), lr=0.001)\n",
    "\n",
    "\n",
    "for epoch in range(200):  # loop over the dataset multiple times\n",
    "\n",
    "    running_loss = 0.0\n",
    "    loss_store_myada = []\n",
    "    for i, (inputs, labels) in enumerate(trainloader, 0):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        if i % 2000 == 1999:    # print every 2000 mini-batches\n",
    "            loss_store_myada.append(running_loss/2000)\n",
    "            print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / 2000:.3f}')\n",
    "            running_loss = 0.0\n",
    "\n",
    "print('Finished Training myAdagrad')\n",
    "\n",
    "\n",
    "\n",
    "##adam training\n",
    "\n",
    "criterion1 = nn.CrossEntropyLoss()\n",
    "#optimizer = optim.Adagrad(net.parameters(), lr=0.001)\n",
    "optimizer1  = optim.Adam(net.parameters(), lr =0.001)\n",
    "#optimizer = optim.SGD(net.parameters(),lr = 0.001,momentum=0.9)\n",
    "#optimizer = myAdagrad(net.parameters(), lr=0.01)\n",
    "#optimizer = SCAdagrad(net.parameters(), lr=0.001)\n",
    "\n",
    "\n",
    "for epoch in range(200):  # loop over the dataset multiple times\n",
    "\n",
    "    running_loss1 = 0.0\n",
    "    loss_store_adam = []\n",
    "    for i, (inputs, labels) in enumerate(trainloader, 0):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion1(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer1.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss1 += loss.item()\n",
    "        if i % 2000 == 1999:    # print every 2000 mini-batches\n",
    "            loss_store_adam.append(running_loss/2000)\n",
    "            print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / 2000:.3f}')\n",
    "            running_loss = 0.0\n",
    "\n",
    "print('Finished Training adam')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
